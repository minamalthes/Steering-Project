{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 – Next word steering\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will showcase the effect of steering on the next word predicted using causal language models (generative LLMs trained to predict the next tokens given a sequence of previous tokens).\n",
    "\n",
    "The notebook includes these :\n",
    "\n",
    "1. [Displaying the next token from a prompt](#display-top-next-token---based-on-a-prompt)\n",
    "2. [Steering the prompt using a steering vector, displaying top tokens now](#display-top-next-tokens---after-steering)\n",
    "3. [Generating longer text based on a prompt and steering](#generate-longer-strings-of-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path to the Functions directory\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Functions')\n",
    "sys.path.append('../Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used in this Notebook:\n",
    "\n",
    "### From \"Next_word_steering.py\":\n",
    "- [initialize_model_and_tokenizer](#importing-python-functions-and-data) - Set the model and tokenizer\n",
    "- [display_next_tokens](#display-top-next-token---based-on-a-prompt) - Print next token predictions\n",
    "- [get_embedding_gpt](#create-steering-vector---using-own-sentences) - Embed a list of strings (create a steering vector)\n",
    "- [get_steering_vector_gpt](#create-steering-vector---using-a-feature-file) - Create steering vector based on a feature file\n",
    "- [display_steered_next_tokens](#display-top-next-tokens---after-steering) - Print next token predictions after steering\n",
    "- [generate_steered_text](#generate-longer-strings-of-text) - Generate longer strings of text based on predictions and steering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Import python functions and set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Next_word_steering import display_next_tokens, display_steered_next_tokens, get_embedding_gpt, get_steering_vector_gpt, initialize_model_and_tokenizer, generate_steered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Display top next token - based on a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input any sentence as the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' man': 0.4267\n",
      "' woman': 0.3539\n",
      "' doctor': 0.0781\n",
      "' physician': 0.0069\n",
      "' feminist': 0.0069\n",
      "' male': 0.0062\n",
      "' lady': 0.0052\n",
      "' Muslim': 0.0031\n",
      "' gentleman': 0.0030\n",
      "' girl': 0.0024\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am a woman, my doctor is a\"\n",
    "\n",
    "display_next_tokens(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create steering vector - using own sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_embedding_gpt` creates an embedding of a list of strings, in the layer wanted.\n",
    "\n",
    "- `normalize (True/False)` normalizes the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_steer = 10\n",
    "steering_coefficient = 50\n",
    "\n",
    "# Example of strings: Women\n",
    "steering_sentences = [\n",
    "    \"She braided her daughter’s hair with one hand while sending an email with the other — and no one questioned it.\",\n",
    "    \"The midwife stood calm as storms, her voice steadier than the monitors beeping beside her.\",\n",
    "    \"Wearing heels or combat boots, she walks like the world owes her space — and it does.\",\n",
    "    \"She bleeds monthly and still runs marathons, meetings, and entire households.\",\n",
    "    \"The senator adjusted her blazer and silenced the room before saying a single word.\",\n",
    "    \"She is the matriarch, the memory-keeper, the one everyone calls when things fall apart.\",\n",
    "    \"Her lipstick is warpaint, and her silence is strategy.\",\n",
    "    \"From nursery rhymes to protest chants, her voice has always carried more than melody.\",\n",
    "    \"She stitched every family story into the quilt that now warms three generations.\",\n",
    "    \"She grew life inside her, lost sleep for years, and still built a business from scratch.\",\n",
    "    \"The grandmother who crossed borders with babies strapped to her chest — that’s who she is.\",\n",
    "    \"She is the girl told to smile, the teen told to shrink, the woman who refused.\",\n",
    "    \"Behind every medal, there's a ponytail soaked in sweat and defiance.\",\n",
    "    \"She signs her name where others once wrote hers for her.\",\n",
    "    \"You can find her in every history book margin — not because she wasn’t there, but because someone tried to erase her.\"]\n",
    "\n",
    "steering_vector = get_embedding_gpt(model, tokenizer, steering_sentences, layer_to_steer, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create steering vector - using a feature file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_steering_vector_gpt` creates a steering vector using the feature wanted and the layer wanted. This function uses the function `get_embedding_gpt` to create the embedding vector of the text.\n",
    "\n",
    "This function uses the function `import_feature_texts(f\"Features/{feature}\")`, and reqires the user to have a folder called \"Features\" with a collection of feature texts inside files called \"feature.txt\" and optionally \"opposite.txt\".\n",
    "\n",
    "- `normalize` normalizes the steering vector (but the get_embedding function is never normalized at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlayer_to_steer = 11\\nsteering_coefficient = 2\\nfeature = \"Love\"\\n\\nsteering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment this cell to use the steering vector from your feature file\n",
    "'''\n",
    "layer_to_steer = 11\n",
    "steering_coefficient = 2\n",
    "feature = \"Love\"\n",
    "\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display top next tokens - after steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`display_steered_next_tokens` steers the prompt using the steering vector, and displays the top \"k\" predictions for the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "Steering coefficient: 50\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' woman': 0.4444\n",
      "' man': 0.3300\n",
      "' doctor': 0.0572\n",
      "' lady': 0.0081\n",
      "' male': 0.0077\n",
      "' feminist': 0.0076\n",
      "' physician': 0.0051\n",
      "' girl': 0.0043\n",
      "' female': 0.0034\n",
      "' gentleman': 0.0030\n"
     ]
    }
   ],
   "source": [
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Generate longer strings of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"In the year 3000, humanity\"\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, \"War\", 8, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_steered_text` steers the prompt using the steering vector and then generates a longer sentence or text using the predictions for the next tokens\n",
    "\n",
    "- `max_tokens (int)` is the maximum amount of tokens that can be generated\n",
    "\n",
    "- `stop_token=\".\"` can make the generation of tokens stop at a \".\" thus creating one sentence, if `stop_token=None`, it stops only when \"max_tokens\" is reached\n",
    "\n",
    "- `temperature (float)` controls randomness: lower = more deterministic, higher = more creative\n",
    "    - **1.0** = No change (baseline)\n",
    "    - **<1.0** = Less random, sharpens distribution, picking more probable words\n",
    "    - **>1.0** = More creative and diverse output, picks less probable words\n",
    "    - **0** = Always picks most probable token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the year 3000, humanity recorded around equivalent\\n\\n\\nhundred-twos should kill an Earth being\\n\\n\\nequ'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_steered_text(model, tokenizer, new_prompt, layer_to_steer, steering_vector, steering_coefficient, stop_token=\".\", max_tokens=20, temperature=1.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
